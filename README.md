# LLM Output Validation with Self-Critique

This repo demonstrates an approach to validating Large Language Model (LLM) outputs using LLM self-critique. 

By using LLMs to analyze and evaluate their own responses, we can significantly improve the reliability of AI-generated structured outputs.

