# LLM Output Validation with Self-Critique

This repo demonstrates an approach to validating Large Language Model (LLM) outputs using LLM self-critique techniques. By using LLMs to analyze and evaluate their own responses, we can improve the reliability of AI-generated structured outputs.

